{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approximating Eigenvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we look at some methods that can be used to approximate the eigenvalues of a matrix $A$.  Although it is possible to find the exact eigenvalues for small matrices, the approach is not practical for moderately large matrices.\n",
    "\n",
    "Most introductory references on the subject demonstrate a direct way to compute eigenvalues of an $n\\times n$ matrix $A$ by computing roots of an associated $n$th polynomial, known as the *characteristic polynomial*.  For example, suppose $A$ is a $2\\times 2$ matrix.\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "A = \\left[ \\begin{array}{rr} a & b  \\\\ c & d \\end{array}\\right]\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "The eigenvalues of $A$ are solutions to the quadratic equation $\\lambda^2 - (a+d)\\lambda + ad-bc = 0$, which can be written explicitly in terms of $a$, $b$, $c$, and $d$ using the quadratic formula.  The challenges with larger matrices are that the polynomial is more difficult to construct, and the roots cannot be easily found with a formula.\n",
    "\n",
    "The algorithms we describe in the section are iterative methods.  They generate a sequence of values $\\{\\lambda^{(1)}, \\lambda^{(2)}, \\lambda^{(3)}, ... \\}$ that approach a true eigenvalue of the matrix under consideration. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Power Method\n",
    "\n",
    "The first algorithm we introduce for approximating eigenvalues is known as the **Power Method**.  This method generates a sequence of vectors by repeated matrix multiplication.  Under suitable conditions, the sequence of vectors approaches the eigenvector associated with the eigenvalue that is largest in absolute value.    \n",
    "\n",
    "For the simplest explanation, suppose that $A$ is an $n\\times n$ diagonalizable matrix with eigenvectors $\\{V_1, V_2, ... V_n\\}$, and that $\\lambda_1$ is the eigenvalue of $A$ that is largest in absolute value.  To begin the Power Method, we choose any nonzero vector and label it $X^{(0)}$.  We can express $X^{(0)}$  as a linear combination of the eigenvectors since they form a basis for $\\mathbb{R}^n$.\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "X^{(0)} = c_1V_1 + c_2V_2 + ... c_nV_n\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "We now form a sequence of vectors $X^{(1)}$, $X^{(2)}$, $X^{(3)}$, ..., by setting $X^{(m)}= AX^{(m-1)}$.  Each of these vectors is easly expressed in terms of the eigenvectors.\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "X^{(1)} = AX^{(0)} & = & c_1AV_1 + c_2AV_2 + ... c_nAV_n \\\\\n",
    "                   & = & c_1\\lambda_1V_1 + c_2\\lambda_2V_2 + ... c_n\\lambda_nV_n \\\\\n",
    "X^{(2)} = AX^{(1)} & = & c_1\\lambda_1AV_1 + c_2\\lambda_2AV_2 + ... c_n\\lambda_nAV_n \\\\\n",
    "                   & = & c_1\\lambda_1^2V_1 + c_2\\lambda_2^2V_2 + ... c_n\\lambda_n^2V_n \\\\\n",
    "                   & \\vdots & \\\\\n",
    "X^{(m)} = AX^{(m-1)} & = & c_1\\lambda_1^{m-1}AV_1 + c_2\\lambda_2^{m-1}AV_2 + ... c_n\\lambda_n^{m-1}AV_n \\\\\n",
    "                   & = & c_1\\lambda_1^mV_1 + c_2\\lambda_2^mV_2 + ... c_n\\lambda_n^mV_n \n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "\n",
    "In the expression for $X^{(m)}$, we can then factor out $\\lambda_1^m$ to understand what happens as $m$ gets large.\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "X^{(m)} =  \\lambda_1^m\\left(c_1V_1 + c_2\\left(\\frac{\\lambda_2}{\\lambda_1}\\right)^mV_2 + ... c_n\\left(\\frac{\\lambda_n}{\\lambda_1}\\right)^mV_n\\right) \n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "If $|\\lambda_1| > |\\lambda_i|$ for all $i\\neq 1$, then $|\\lambda_i/\\lambda_1|< 1$ and $(\\lambda_i/\\lambda_1)^m$ will approach zero as $m$ gets large.  This means that if we repeatedly multiply a vector by the matrix $A$, eventually we will get a vector that is very nearly in the direction of the eigenvector that corresponds to the $\\lambda_1$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's demonstrate the calculation on the matrix shown here before we discuss further.\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "A = \\left[ \\begin{array}{rrrr} -2 & 6 & 2 & -8 \\\\ -6 & 0 & 12 & 12 \\\\ -6 & 0 & 12 & 12 \\\\ -10 & 3 & 7 & 14 \\end{array}\\right]\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "As a matter of practicality, it is common to scale the vectors in the sequence to unit length when as the Power Method is applied.  Since all components of the vectors get divided by the same factor, this doesn't change the ultimate behavior of the sequence.  They still approach the direction of the eigenvector, but do not grow in magnitude due to the factor $\\lambda_1^m$. \n",
    "\n",
    "We calculate $X^{(20)}$ below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7.87666229e-13]\n",
      " [5.77350269e-01]\n",
      " [5.77350269e-01]\n",
      " [5.77350269e-01]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import laguide as lag\n",
    "A = np.array([[-2, 6, 2, -8],[-6, 0, 12, 12],[-6, 0, 12, 12],[-10, 3, 7, 14]])\n",
    "X = np.array([[1],[1],[1],[1]])\n",
    "\n",
    "m = 1\n",
    "while (m < 21):\n",
    "    X = A@X\n",
    "    X = X/lag.Magnitude(X)\n",
    "    m = m + 1\n",
    "    \n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if $X$ is the eigenvector of $A$ with unit magnitude, then $|AX| = |\\lambda_1X| = |\\lambda_1|$.  We can therefor approximate $|\\lambda_1|$ with $|AX|$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.999999999989992\n"
     ]
    }
   ],
   "source": [
    "print(lag.Magnitude(A@X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that 24 is an estimate for $\\lambda_1$.   To determine if our calculation is correct, we can compare $AX$ with $\\lambda_1X$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.04791739e-11]\n",
      " [-4.72688555e-12]\n",
      " [-4.72688555e-12]\n",
      " [-7.87814258e-12]]\n"
     ]
    }
   ],
   "source": [
    "print(A@X - 24*X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed the difference $AX-24X$ small.  Note that in this case, we can even do the calculation with integer multiplication.  Notice that $X$ has 0 in the first entry and the other entries are equal.  If we set these entries to 1, the result is easy to calculate even without the aid of the computer.  (*Remember that we can change the magnitude of an eigenvector and it is still an eigenvector.*) \n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "AX = \\left[ \\begin{array}{rrrr} -2 & 6 & 2 & -8 \\\\ -6 & 0 & 12 & 12 \\\\ -6 & 0 & 12 & 12 \\\\ -10 & 3 & 7 & 14 \\end{array}\\right]\n",
    "\\left[ \\begin{array}{r} 0 \\\\ 1\\\\ 1 \\\\ 1 \\end{array}\\right] =\n",
    "\\left[ \\begin{array}{r} 0 \\\\ 24\\\\ 24 \\\\ 24 \\end{array}\\right] = 24X\n",
    "\\end{equation}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.40000000e+01+0.j -6.00000000e+00+0.j  6.00000000e+00+0.j\n",
      " -3.99305433e-15+0.j]\n",
      "[ 4.94568128e-16 -5.77350269e-01 -5.77350269e-01 -5.77350269e-01]\n"
     ]
    }
   ],
   "source": [
    "import scipy.linalg as SLA\n",
    "\n",
    "evalues, evectors = SLA.eig(A)\n",
    "print(evalues)\n",
    "print(evectors[:,0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
