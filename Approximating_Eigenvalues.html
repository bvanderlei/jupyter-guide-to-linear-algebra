
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Approximating Eigenvalues &#8212; Jupyter Guide to Linear Algebra</title>
    
  <link rel="stylesheet" href="_static/css/index.f658d18f9b420779cfdf24aa0a7e2d77.css">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/sphinx-book-theme.c441f2ba0852f4cabcb80105e3a46ae6.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.d3f166471bb80abb5163.js">

    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.7d483ff0a819d6edff12ce0b1ead3928.js"></script>
    <script src="_static/sphinx-book-theme.be0a4a0c39cd630af62a2fcf693f3f06.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe,.cell"
        const thebe_selector_input = "pre,.cell_input div.highlight"
        const thebe_selector_output = ".output,.cell_output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Applications of Eigenvalues and Eigenvectors" href="Applications_EV.html" />
    <link rel="prev" title="Diagonalization" href="Diagonalization.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="index.html">
  
  <img src="_static/reflection_logo.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Jupyter Guide to Linear Algebra</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="intro.html">
   Welcome to the Jupyter Guide to Linear Algebra
  </a>
 </li>
</ul>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="Jupyter_Introduction.html">
   Introduction to Jupyter
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="Python_Introduction.html">
     Introduction to Python
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Numpy_Introduction.html">
     Introduction to NumPy and Matplotlib
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="Linear_Systems.html">
   Linear Systems
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="Gaussian_Elimination.html">
     Gaussian Elimination
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Matrix_Algebra.html">
     Matrix Algebra
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Solving_Systems.html">
     Solving Systems using Elimination
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Inverse_Matrices.html">
     Inverse Matrices
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="LU_Factorization.html">
     LU Factorization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Applications.html">
     Applications of Linear Systems and Matrix Algebra
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="Vector_Spaces.html">
   Vector Spaces
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="General_Linear_Systems.html">
     General Linear Systems
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Linear_Combinations.html">
     Linear Combinations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Linear_Independence.html">
     Linear Independence
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Bases.html">
     Bases
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Vector_Space_Examples.html">
     Vector Space Examples
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Applications_VS.html">
     Applications of Vector Spaces
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="Linear_Transformations.html">
   Linear Transformations
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="Matrix_Representations.html">
     Matrix Representations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Planar_Transformations.html">
     Transformations in a Plane
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Applications_LT.html">
     Applications of Linear Transformations
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="Inner_Products.html">
   Inner Products
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="Orthogonalization.html">
     Orthogonalization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="QR_Factorization.html">
     QR Factorization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Orthogonal_Subspaces.html">
     Orthogonal Subspaces
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Least_Squares_Solutions.html">
     Least Squares Solutions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Projections.html">
     Projections
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Applications_IP.html">
     Applications of Inner Products
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 current active collapsible-parent">
  <a class="reference internal" href="Eigenvalues.html">
   Eigenvalues
  </a>
  <ul class="current collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="Diagonalization.html">
     Diagonalization
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Approximating Eigenvalues
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Applications_EV.html">
     Applications of Eigenvalues and Eigenvectors
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="Solutions.html">
   Solutions
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="Introduction_Solutions.html">
     Introduction to Jupyter
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Linear_Systems_Solutions.html">
     Linear Systems
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/Approximating_Eigenvalues.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->


            <!-- Full screen (wrap in <a> to have style consistency -->
            <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                    data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                    title="Fullscreen mode"><i
                        class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/bvanderlei/jupyter-guide-to-linear-algebra/main?urlpath=tree/Approximating_Eigenvalues.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        <button type="button" class="btn btn-secondary topbarbtn"
            onclick="initThebeSBT()" title="Launch Thebe" data-toggle="tooltip" data-placement="left"><i
                class="fas fa-play"></i><span style="margin-left: .4em;">Live Code</span></button>
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i>
            Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#power-method">
   Power method
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#inverse-power-method">
   Inverse power method
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#shifts">
     Shifts
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#exercises">
   Exercises
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references">
   References
  </a>
 </li>
</ul>

        </nav>
        
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="approximating-eigenvalues">
<h1>Approximating Eigenvalues<a class="headerlink" href="#approximating-eigenvalues" title="Permalink to this headline">¶</a></h1>
<p>In this section we look at some methods that can be used to approximate the eigenvalues of a matrix <span class="math notranslate nohighlight">\(A\)</span>.  Although it is possible to find the exact eigenvalues for small matrices, the approach is impractical for larger matrices.</p>
<p>Most introductory textbooks demonstrate a direct way to compute eigenvalues of an <span class="math notranslate nohighlight">\(n\times n\)</span> matrix <span class="math notranslate nohighlight">\(A\)</span> by computing roots of an associated <span class="math notranslate nohighlight">\(n\)</span>th degree polynomial, known as the <em>characteristic polynomial</em>.  For example, suppose <span class="math notranslate nohighlight">\(A\)</span> is a <span class="math notranslate nohighlight">\(2\times 2\)</span> matrix.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{equation}
A = \left[ \begin{array}{rr} a &amp; b  \\ c &amp; d \end{array}\right]
\end{equation}
\end{split}\]</div>
<p>The eigenvalues of <span class="math notranslate nohighlight">\(A\)</span> are solutions to the quadratic equation <span class="math notranslate nohighlight">\(\lambda^2 - (a+d)\lambda + ad-bc = 0\)</span>, which can be written explicitly in terms of <span class="math notranslate nohighlight">\(a\)</span>, <span class="math notranslate nohighlight">\(b\)</span>, <span class="math notranslate nohighlight">\(c\)</span>, and <span class="math notranslate nohighlight">\(d\)</span> using the quadratic formula.  The challenges with larger matrices are that the polynomial is more difficult to construct, and the roots cannot be easily found with a formula.</p>
<p>The algorithms we describe in the section are iterative methods.  They generate a sequence of vectors <span class="math notranslate nohighlight">\(\{X^{(1)}, X^{(2)}, X^{(3)}, ... \}\)</span> that approach a true eigenvector of the matrix under consideration.  An approximation of the corresponding eigenvalue can then be computed by multiplying the approximate eigenvector by <span class="math notranslate nohighlight">\(A\)</span>.</p>
<div class="section" id="power-method">
<h2>Power method<a class="headerlink" href="#power-method" title="Permalink to this headline">¶</a></h2>
<p>The first algorithm we introduce for approximating eigenvalues is known as the <strong>Power Method</strong>.  This method generates a sequence of vectors by repeated matrix multiplication.  Under suitable conditions, the sequence of vectors approaches the eigenvector associated with the eigenvalue that is largest in absolute value.</p>
<p>For the simplest explanation, suppose that <span class="math notranslate nohighlight">\(A\)</span> is an <span class="math notranslate nohighlight">\(n\times n\)</span> diagonalizable matrix with eigenvectors <span class="math notranslate nohighlight">\(\{V_1, V_2, ... V_n\}\)</span>, and that <span class="math notranslate nohighlight">\(\lambda_1\)</span> is the eigenvalue of <span class="math notranslate nohighlight">\(A\)</span> that is largest in absolute value.  To begin the Power Method, we choose any nonzero vector and label it <span class="math notranslate nohighlight">\(X^{(0)}\)</span>.  We can express <span class="math notranslate nohighlight">\(X^{(0)}\)</span>  as a linear combination of the eigenvectors since they form a basis for <span class="math notranslate nohighlight">\(\mathbb{R}^n\)</span>.</p>
<div class="math notranslate nohighlight">
\[
\begin{equation}
X^{(0)} = c_1V_1 + c_2V_2 + ... c_nV_n
\end{equation}
\]</div>
<p>We now form a sequence of vectors <span class="math notranslate nohighlight">\(X^{(1)}\)</span>, <span class="math notranslate nohighlight">\(X^{(2)}\)</span>, <span class="math notranslate nohighlight">\(X^{(3)}\)</span>, …, by setting <span class="math notranslate nohighlight">\(X^{(m)}= AX^{(m-1)}\)</span>.  Each of these vectors is also easly expressed in terms of the eigenvectors.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{eqnarray*}
X^{(1)} = AX^{(0)} &amp; = &amp; c_1AV_1 + c_2AV_2 + ... c_nAV_n \\
                   &amp; = &amp; c_1\lambda_1V_1 + c_2\lambda_2V_2 + ... c_n\lambda_nV_n \\
X^{(2)} = AX^{(1)} &amp; = &amp; c_1\lambda_1AV_1 + c_2\lambda_2AV_2 + ... c_n\lambda_nAV_n \\
                   &amp; = &amp; c_1\lambda_1^2V_1 + c_2\lambda_2^2V_2 + ... c_n\lambda_n^2V_n \\
                   &amp; \vdots &amp; \\
X^{(m)} = AX^{(m-1)} &amp; = &amp; c_1\lambda_1^{m-1}AV_1 + c_2\lambda_2^{m-1}AV_2 + ... c_n\lambda_n^{m-1}AV_n \\
                   &amp; = &amp; c_1\lambda_1^mV_1 + c_2\lambda_2^mV_2 + ... c_n\lambda_n^mV_n 
\end{eqnarray*}
\end{split}\]</div>
<p>In the expression for <span class="math notranslate nohighlight">\(X^{(m)}\)</span>, we can then factor out <span class="math notranslate nohighlight">\(\lambda_1^m\)</span> to understand what happens as <span class="math notranslate nohighlight">\(m\)</span> gets large.</p>
<div class="math notranslate nohighlight">
\[
\begin{equation}
X^{(m)} =  \lambda_1^m\left(c_1V_1 + c_2\left(\frac{\lambda_2}{\lambda_1}\right)^mV_2 + ... c_n\left(\frac{\lambda_n}{\lambda_1}\right)^mV_n\right) 
\end{equation}
\]</div>
<p>If <span class="math notranslate nohighlight">\(|\lambda_1| &gt; |\lambda_i|\)</span> for all <span class="math notranslate nohighlight">\(i\neq 1\)</span>, then <span class="math notranslate nohighlight">\(|\lambda_i/\lambda_1|&lt; 1\)</span> and <span class="math notranslate nohighlight">\((\lambda_i/\lambda_1)^m\)</span> will approach zero as <span class="math notranslate nohighlight">\(m\)</span> gets large.  This means that if we repeatedly multiply a vector by the matrix <span class="math notranslate nohighlight">\(A\)</span>, eventually we will get a vector that is very nearly in the direction of the eigenvector that corresponds to the <span class="math notranslate nohighlight">\(\lambda_1\)</span>.</p>
<p>Let’s demonstrate the calculation on the matrix shown here before we discuss the method further.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{equation}
A = \left[ \begin{array}{rrrr} -2 &amp; 6 &amp; 2 &amp; -8 \\ -6 &amp; 0 &amp; 12 &amp; 12 \\ -6 &amp; 0 &amp; 12 &amp; 12 \\ -10 &amp; 3 &amp; 7 &amp; 14 \end{array}\right]
\end{equation}
\end{split}\]</div>
<p>As a matter of practicality, it is common to scale the vectors in the sequence to unit length as the Power Method is applied.  If the vectors in the sequence are not scaled, their magnitudes will grow of <span class="math notranslate nohighlight">\(\lambda_1&gt;1\)</span> or decay if <span class="math notranslate nohighlight">\(\lambda_1&lt;1\)</span>.    Since all components of the vectors get divided by the same factor when the vector is scaled, this step doesn’t change the ultimate behavior of the sequence.  The scaled sequence of vectors still approaches the direction of the eigenvector.</p>
<p>We choose an arbitrary <span class="math notranslate nohighlight">\(X^{(0)}\)</span> and calculate <span class="math notranslate nohighlight">\(X^{(20)}\)</span> using the following rule.</p>
<div class="math notranslate nohighlight">
\[
\begin{equation}
X^{(m)}=\frac{AX^{(m-1)}}{||AX^{(m-1)}||}
\end{equation}
\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">laguide</span> <span class="k">as</span> <span class="nn">lag</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">8</span><span class="p">],[</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">12</span><span class="p">],[</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">12</span><span class="p">],[</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">14</span><span class="p">]])</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">],[</span><span class="mi">0</span><span class="p">],[</span><span class="mi">0</span><span class="p">],[</span><span class="mi">0</span><span class="p">]])</span>

<span class="n">m</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">while</span> <span class="p">(</span><span class="n">m</span> <span class="o">&lt;</span> <span class="mi">20</span><span class="p">):</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">A</span><span class="nd">@X</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">/</span><span class="n">lag</span><span class="o">.</span><span class="n">Magnitude</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">m</span> <span class="o">+</span> <span class="mi">1</span>
    
<span class="nb">print</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[ 1.57523994e-12]
 [-5.77350269e-01]
 [-5.77350269e-01]
 [-5.77350269e-01]]
</pre></div>
</div>
</div>
</div>
<p>Now if <span class="math notranslate nohighlight">\(X\)</span> is the eigenvector of <span class="math notranslate nohighlight">\(A\)</span> with unit magnitude, then <span class="math notranslate nohighlight">\(|AX| = |\lambda_1X| = |\lambda_1|\)</span>.  We can therefore approximate <span class="math notranslate nohighlight">\(|\lambda_1|\)</span> with <span class="math notranslate nohighlight">\(|AX|\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">lag</span><span class="o">.</span><span class="n">Magnitude</span><span class="p">(</span><span class="n">A</span><span class="nd">@X</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>24.000000000020005
</pre></div>
</div>
</div>
</div>
<p>It appears that 24 is an estimate for <span class="math notranslate nohighlight">\(\lambda_1\)</span>.   To determine if our calculation is correct, we can compare <span class="math notranslate nohighlight">\(AX\)</span> with <span class="math notranslate nohighlight">\(\lambda_1X\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">A</span><span class="nd">@X</span> <span class="o">-</span> <span class="mi">24</span><span class="o">*</span><span class="n">X</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[-4.09570156e-11]
 [-9.45021839e-12]
 [-9.45021839e-12]
 [-1.57545088e-11]]
</pre></div>
</div>
</div>
</div>
<p>Indeed the difference <span class="math notranslate nohighlight">\(AX-24X\)</span> small.  Note that in this case, we can even do the calculation with integer multiplication.  Notice that <span class="math notranslate nohighlight">\(X\)</span> has 0 in the first entry and the other entries are equal.  If we set these entries to 1, the result is easy to calculate even without the aid of the computer.  (<em>Remember that we can change the magnitude of an eigenvector and it is still an eigenvector.</em>)</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{equation}
AX = \left[ \begin{array}{rrrr} -2 &amp; 6 &amp; 2 &amp; -8 \\ -6 &amp; 0 &amp; 12 &amp; 12 \\ -6 &amp; 0 &amp; 12 &amp; 12 \\ -10 &amp; 3 &amp; 7 &amp; 14 \end{array}\right]
\left[ \begin{array}{r} 0 \\ 1\\ 1 \\ 1 \end{array}\right] =
\left[ \begin{array}{r} 0 \\ 24\\ 24 \\ 24 \end{array}\right] = 24X
\end{equation}
\end{split}\]</div>
<p>In practice, we do not know how many iterations we need to perform in order to get a good approximation of the eigenvector eigenvalue.  Instead we should specify a condition upon which we will be satisfied with the approximation and terminate the iteration.  For example, since <span class="math notranslate nohighlight">\(||AX^{(m)}||\approx \lambda_1\)</span> and <span class="math notranslate nohighlight">\(AX^{(m)}\approx \lambda_1X^{(m)}\)</span> we might require that <span class="math notranslate nohighlight">\(AX^{(m)} - ||AX^{(m)}||X^{(m)} &lt; \epsilon\)</span> for some small number <span class="math notranslate nohighlight">\(\epsilon\)</span> known as a tolerance.  This condition ensures that <span class="math notranslate nohighlight">\(X^{(m)}\)</span> functions roughly like an eigenvector.  It is also best to include in the code a limit on the number of iterations that will be carried out.  This ensures that the computation will eventually end, even if a satisfactory result has not yet been achieved.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">],[</span><span class="mi">0</span><span class="p">],[</span><span class="mi">0</span><span class="p">],[</span><span class="mi">0</span><span class="p">]])</span>

<span class="n">m</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">tolerance</span> <span class="o">=</span> <span class="mf">0.0001</span>
<span class="n">MAX_ITERATIONS</span> <span class="o">=</span> <span class="mi">100</span>

<span class="c1">## Compute difference in stopping condition</span>
<span class="c1">## Assign Y = AX to avoid computing AX multiple times</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">A</span><span class="nd">@X</span>
<span class="n">difference</span> <span class="o">=</span> <span class="n">Y</span> <span class="o">-</span> <span class="n">lag</span><span class="o">.</span><span class="n">Magnitude</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span><span class="o">*</span><span class="n">X</span>

<span class="k">while</span> <span class="p">(</span><span class="n">m</span> <span class="o">&lt;</span> <span class="n">MAX_ITERATIONS</span> <span class="ow">and</span> <span class="n">lag</span><span class="o">.</span><span class="n">Magnitude</span><span class="p">(</span><span class="n">difference</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">tolerance</span><span class="p">):</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">Y</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">/</span><span class="n">lag</span><span class="o">.</span><span class="n">Magnitude</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

    <span class="c1">## Compute difference in stopping condition</span>
    <span class="n">Y</span> <span class="o">=</span> <span class="n">A</span><span class="nd">@X</span>
    <span class="n">difference</span> <span class="o">=</span> <span class="n">Y</span> <span class="o">-</span> <span class="n">lag</span><span class="o">.</span><span class="n">Magnitude</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span><span class="o">*</span><span class="n">X</span>
    
    <span class="n">m</span> <span class="o">=</span> <span class="n">m</span> <span class="o">+</span> <span class="mi">1</span>
    
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Eigenvector is approximately:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Magnitude of the eigenvalue is approximately:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">lag</span><span class="o">.</span><span class="n">Magnitude</span><span class="p">(</span><span class="n">Y</span><span class="p">),</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Magnitude of the difference is:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">lag</span><span class="o">.</span><span class="n">Magnitude</span><span class="p">(</span><span class="n">difference</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Eigenvector is approximately:
[[ 1.65181395e-06]
 [-5.77350269e-01]
 [-5.77350269e-01]
 [-5.77350269e-01]] 

Magnitude of the eigenvalue is approximately:
24.00002098082306 

Magnitude of the difference is:
4.3284704409694366e-05
</pre></div>
</div>
</div>
</div>
<p>A more common condition to require is that <span class="math notranslate nohighlight">\(||X^{(m)} - X^{(m-1})|| &lt; \epsilon\)</span> for a given tolerance <span class="math notranslate nohighlight">\(\epsilon\)</span>.  This condition mere requires that the vectors in the sequence get close to one another, not that they are actually approximate an eigenvector.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">],[</span><span class="mi">0</span><span class="p">],[</span><span class="mi">0</span><span class="p">],[</span><span class="mi">0</span><span class="p">]])</span>

<span class="n">m</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">tolerance</span> <span class="o">=</span> <span class="mf">0.0001</span>
<span class="n">MAX_ITERATIONS</span> <span class="o">=</span> <span class="mi">100</span>

<span class="n">difference</span> <span class="o">=</span> <span class="n">X</span>

<span class="k">while</span> <span class="p">(</span><span class="n">m</span> <span class="o">&lt;</span> <span class="n">MAX_ITERATIONS</span> <span class="ow">and</span> <span class="n">lag</span><span class="o">.</span><span class="n">Magnitude</span><span class="p">(</span><span class="n">difference</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">tolerance</span><span class="p">):</span>
    <span class="n">X_previous</span> <span class="o">=</span> <span class="n">X</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">A</span><span class="nd">@X</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">/</span><span class="n">lag</span><span class="o">.</span><span class="n">Magnitude</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

    <span class="c1">## Compute difference in stopping condition</span>
    <span class="n">difference</span> <span class="o">=</span> <span class="n">X</span> <span class="o">-</span> <span class="n">X_previous</span>
    
    <span class="n">m</span> <span class="o">=</span> <span class="n">m</span> <span class="o">+</span> <span class="mi">1</span>
    
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Eigenvector is approximately:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Magnitude of the eigenvalue is approximately:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">lag</span><span class="o">.</span><span class="n">Magnitude</span><span class="p">(</span><span class="n">Y</span><span class="p">),</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Magnitude of the difference is:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">lag</span><span class="o">.</span><span class="n">Magnitude</span><span class="p">(</span><span class="n">difference</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Eigenvector is approximately:
[[ 2.64294012e-05]
 [-5.77350269e-01]
 [-5.77350269e-01]
 [-5.77350269e-01]] 

Magnitude of the eigenvalue is approximately:
24.00002098082306 

Magnitude of the difference is:
8.434774776937697e-05
</pre></div>
</div>
</div>
</div>
<p>While the Power Method is easy to understand and apply, it does have disadvantages.  The most apparent disadvantage is that the method only applies to the largest eigenvalue.  This is not a huge detriment since applications often only require an approximation of the largest eigenvalue.  Also, as we will demonstrate below, it is possible to easily modify the method to approximate the other eigenvalues.  A more significant disadvantage is that the rate at which the sequence converges can be slow in some circumstances.  For example, we can see that if <span class="math notranslate nohighlight">\(|\lambda_1|\)</span> is close to <span class="math notranslate nohighlight">\(|\lambda_2|\)</span>, then <span class="math notranslate nohighlight">\(|\lambda_1/\lambda_2|^m\)</span> approaches zero more slowly as <span class="math notranslate nohighlight">\(m\)</span> gets large.  The Power Method may fail to converge at all if <span class="math notranslate nohighlight">\(|\lambda_1| = |\lambda_2|\)</span>, which occurs if <span class="math notranslate nohighlight">\(\lambda_1 = -\lambda_2\)</span>, or if <span class="math notranslate nohighlight">\(\lambda_1\)</span> and <span class="math notranslate nohighlight">\(\lambda_2\)</span> are a complex conjugate pair.  Additionally, the method may perform poorly if the <span class="math notranslate nohighlight">\(V_1\)</span> component of <span class="math notranslate nohighlight">\(X^{(0)}\)</span> is too small.</p>
</div>
<div class="section" id="inverse-power-method">
<h2>Inverse power method<a class="headerlink" href="#inverse-power-method" title="Permalink to this headline">¶</a></h2>
<p>The <strong>Inverse Power Method</strong> is a modified version of the Power Method that allows us to approximate eigenvalues that are <em>not the largest</em>.  All that is needed to make the modification is two simple facts that relate changes in a matrix to changes in the eigenvalues of that matrix.  Let’s suppose that <span class="math notranslate nohighlight">\(A\)</span> is an invertible <span class="math notranslate nohighlight">\(n\times n\)</span> matrix with eigenvalue <span class="math notranslate nohighlight">\(\lambda\)</span> and corresponding eigenvector <span class="math notranslate nohighlight">\(V\)</span>, so that <span class="math notranslate nohighlight">\(AV=\lambda V\)</span>.  If we multiply this equation by <span class="math notranslate nohighlight">\(A^{-1}\)</span>, we get <span class="math notranslate nohighlight">\(V=\lambda A^{-1}V\)</span>, which can then be divided by <span class="math notranslate nohighlight">\(\lambda\)</span> to illustrate the useful fact.</p>
<div class="math notranslate nohighlight">
\[
\begin{equation}
A^{-1}V = \frac{1}{\lambda}V
\end{equation}
\]</div>
<p>If <span class="math notranslate nohighlight">\(\lambda\)</span> is an eigenvalue of <span class="math notranslate nohighlight">\(A\)</span>, then <span class="math notranslate nohighlight">\(\lambda^{-1}\)</span> is an eigenvalue of <span class="math notranslate nohighlight">\(A\)</span>.  Furthermore the eigenvector of <span class="math notranslate nohighlight">\(A\)</span> is also an eigenvector of <span class="math notranslate nohighlight">\(A^{-1}\)</span>.  The important point here is that if <span class="math notranslate nohighlight">\(\lambda_n\)</span> is the smallest eigenvalue of <span class="math notranslate nohighlight">\(A\)</span>, then <span class="math notranslate nohighlight">\(\lambda_n^{-1}\)</span> is the <em>largest</em> eigenvector of <span class="math notranslate nohighlight">\(A^{-1}\)</span>.  If we want to approximate the smallest eigenvalue of <span class="math notranslate nohighlight">\(A\)</span>, we can just apply the Power Method to <span class="math notranslate nohighlight">\(A^{-1}\)</span>.</p>
<p>We demonstrate the calculation for the following <span class="math notranslate nohighlight">\(3\times 3\)</span> matrix.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{equation}
A = \left[ \begin{array}{rrrr} 9 &amp; -1 &amp; -3 \\ 0 &amp; 6 &amp; 0 \\ -6 &amp; 3 &amp; 6 \end{array}\right]
\end{equation}
\end{split}\]</div>
<p>Again we choose an arbitrary <span class="math notranslate nohighlight">\(X^{(0)}\)</span>, and generate a sequence of vectors by multiplying by <span class="math notranslate nohighlight">\(A^{-1}\)</span> and scaling the result to unit length.</p>
<div class="math notranslate nohighlight">
\[
\begin{equation}
X^{(m)}=\frac{A^{-1}X^{(m-1)}}{||A^{-1}X^{(m-1)}||}
\end{equation}
\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">],[</span><span class="mi">1</span><span class="p">],[</span><span class="mi">0</span><span class="p">]])</span>

<span class="n">m</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">tolerance</span> <span class="o">=</span> <span class="mf">0.0001</span>
<span class="n">MAX_ITERATIONS</span> <span class="o">=</span> <span class="mi">100</span>

<span class="n">difference</span> <span class="o">=</span> <span class="n">X</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">9</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">3</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">0</span><span class="p">],[</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">6</span><span class="p">]])</span>
<span class="n">A_inv</span> <span class="o">=</span> <span class="n">lag</span><span class="o">.</span><span class="n">Inverse</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>

<span class="k">while</span> <span class="p">(</span><span class="n">m</span> <span class="o">&lt;</span> <span class="n">MAX_ITERATIONS</span> <span class="ow">and</span> <span class="n">lag</span><span class="o">.</span><span class="n">Magnitude</span><span class="p">(</span><span class="n">difference</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">tolerance</span><span class="p">):</span>
    <span class="n">X_previous</span> <span class="o">=</span> <span class="n">X</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">A_inv</span><span class="nd">@X</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">/</span><span class="n">lag</span><span class="o">.</span><span class="n">Magnitude</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

    <span class="c1">## Compute difference in stopping condition</span>
    <span class="n">difference</span> <span class="o">=</span> <span class="n">X</span> <span class="o">-</span> <span class="n">X_previous</span>
    
    <span class="n">m</span> <span class="o">=</span> <span class="n">m</span> <span class="o">+</span> <span class="mi">1</span>
    
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Eigenvector is approximately:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Magnitude of the eigenvalue is A inverse is approximately:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">lag</span><span class="o">.</span><span class="n">Magnitude</span><span class="p">(</span><span class="n">A_inv</span><span class="nd">@X</span><span class="p">),</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Magnitude of the eigenvalue is A is approximately:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">lag</span><span class="o">.</span><span class="n">Magnitude</span><span class="p">(</span><span class="n">A</span><span class="nd">@X</span><span class="p">),</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Eigenvector is approximately:
[[-4.47193123e-01]
 [ 6.14168469e-05]
 [-8.94437425e-01]] 

Magnitude of the eigenvalue is A inverse is approximately:
0.3333371476391265 

Magnitude of the eigenvalue is A is approximately:
2.999931351114087 
</pre></div>
</div>
</div>
</div>
<p>The exact value of the smallest eigenvalue of <span class="math notranslate nohighlight">\(A\)</span> is 3, which again can be verified by calculation.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{equation}
AV = \left[ \begin{array}{rrrr} 9 &amp; -1 &amp; -3 \\ 0 &amp; 6 &amp; 0 \\ -6 &amp; 3 &amp; 6 \end{array}\right]
\left[ \begin{array}{r} 1 \\ 0\\ 2 \end{array}\right] =
\left[ \begin{array}{r} 3 \\ 0 \\ 6 \end{array}\right] = 3V
\end{equation}
\end{split}\]</div>
<p>In our discussion of <a class="reference internal" href="Inverse_Matrices.html"><span class="doc std std-doc">Inverse Matrices</span></a> we noted that the construction of an inverse matrix is quite expensive since it requires the solution of <span class="math notranslate nohighlight">\(n\)</span> systems of size <span class="math notranslate nohighlight">\(n\times n\)</span>.  An alternative to constructing <span class="math notranslate nohighlight">\(A^{-1}\)</span> and computing the  <span class="math notranslate nohighlight">\(X^{(m)}=A^{-1}X^{(m-1)}\)</span> is to solve the system <span class="math notranslate nohighlight">\(AX^{(m)}=X^{(m-1)}\)</span> to obtain <span class="math notranslate nohighlight">\(X^{(m)}\)</span>.  This means that we solve one <span class="math notranslate nohighlight">\(n\times n\)</span> system for every iteration.  This appears to require more work than the construction of <span class="math notranslate nohighlight">\(A^{-1}\)</span>, but in fact it is less since every system involves the same coefficient matrix.  We can therefore save much work by performing elimination only once and storing the result in an <span class="math notranslate nohighlight">\(LU\)</span> factorization.  With the the matrix <span class="math notranslate nohighlight">\(A\)</span> factored, each system <span class="math notranslate nohighlight">\(AX^{(m)}=X^{(m-1)}\)</span> only requires one forward substitution and one backward substitution.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scipy.linalg</span> <span class="k">as</span> <span class="nn">sla</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">],[</span><span class="mi">1</span><span class="p">],[</span><span class="mi">0</span><span class="p">]])</span>

<span class="n">m</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">tolerance</span> <span class="o">=</span> <span class="mf">0.0001</span>
<span class="n">MAX_ITERATIONS</span> <span class="o">=</span> <span class="mi">100</span>

<span class="n">difference</span> <span class="o">=</span> <span class="n">X</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">9</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">3</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">0</span><span class="p">],[</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">6</span><span class="p">]])</span>
<span class="n">LU_factorization</span> <span class="o">=</span> <span class="n">sla</span><span class="o">.</span><span class="n">lu_factor</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>

<span class="k">while</span> <span class="p">(</span><span class="n">m</span> <span class="o">&lt;</span> <span class="n">MAX_ITERATIONS</span> <span class="ow">and</span> <span class="n">lag</span><span class="o">.</span><span class="n">Magnitude</span><span class="p">(</span><span class="n">difference</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">tolerance</span><span class="p">):</span>
    <span class="n">X_previous</span> <span class="o">=</span> <span class="n">X</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">sla</span><span class="o">.</span><span class="n">lu_solve</span><span class="p">(</span><span class="n">LU_factorization</span><span class="p">,</span><span class="n">X</span><span class="p">)</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">/</span><span class="n">lag</span><span class="o">.</span><span class="n">Magnitude</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">difference</span> <span class="o">=</span> <span class="n">X</span> <span class="o">-</span> <span class="n">X_previous</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">m</span> <span class="o">+</span> <span class="mi">1</span>
  
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Eigenvector is approximately:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Magnitude of the eigenvalue of A inverse is approximately:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">lag</span><span class="o">.</span><span class="n">Magnitude</span><span class="p">(</span><span class="n">sla</span><span class="o">.</span><span class="n">lu_solve</span><span class="p">(</span><span class="n">LU_factorization</span><span class="p">,</span><span class="n">X</span><span class="p">)),</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Magnitude of the eigenvalue is A is approximately:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">lag</span><span class="o">.</span><span class="n">Magnitude</span><span class="p">(</span><span class="n">A</span><span class="nd">@X</span><span class="p">),</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Eigenvector is approximately:
[[-4.47193123e-01]
 [ 6.14168469e-05]
 [-8.94437425e-01]] 

Magnitude of the eigenvalue of A inverse is approximately:
0.3333371476391265 

Magnitude of the eigenvalue is A is approximately:
2.999931351114087 
</pre></div>
</div>
</div>
</div>
<div class="section" id="shifts">
<h3>Shifts<a class="headerlink" href="#shifts" title="Permalink to this headline">¶</a></h3>
<p>Using a small modification to the Inverse Power Method, we can also approximate eigenvalues that are not the smallest.  For this variation of the method, we need to observe that if we “shift” the diagonal entries of a matrix by a scalar <span class="math notranslate nohighlight">\(\mu\)</span>, all of the eigenvalues of the matrix are also shifted by <span class="math notranslate nohighlight">\(\mu\)</span>.  Let <span class="math notranslate nohighlight">\(A\)</span> be an <span class="math notranslate nohighlight">\(n\times n\)</span> matrix with eigenvalue <span class="math notranslate nohighlight">\(\lambda\)</span> and corresponding eigenvector <span class="math notranslate nohighlight">\(V\)</span>, so that <span class="math notranslate nohighlight">\(AV=\lambda V\)</span>.  Then <span class="math notranslate nohighlight">\((A-\mu I)V = AV - \mu V = \lambda V - \mu V = (\lambda-\mu)V\)</span>, which means that <span class="math notranslate nohighlight">\(V\)</span> is also an eigenvector of the matrix <span class="math notranslate nohighlight">\((A-\mu I)\)</span> corresponding to the eigenvalue <span class="math notranslate nohighlight">\(\lambda -\mu\)</span>.</p>
<div class="math notranslate nohighlight">
\[
\begin{equation}
\frac{1}{\lambda_1-\mu}, \frac{1}{\lambda_2-\mu}, \frac{1}{\lambda_3-\mu}, ....,\frac{1}{\lambda_n-\mu} 
\end{equation}
\]</div>
<p>This is useful because it allows us to now use the Inverse Power Method to approximate the eigenvalue of <span class="math notranslate nohighlight">\(A\)</span> that lies closest to <span class="math notranslate nohighlight">\(\mu\)</span>.  For example, if <span class="math notranslate nohighlight">\(\mu\)</span> is closest to <span class="math notranslate nohighlight">\(\lambda_2\)</span>, then <span class="math notranslate nohighlight">\(|\lambda_2-\mu| &lt; |\lambda_i -\mu|\)</span> for all other <span class="math notranslate nohighlight">\(i\neq 2\)</span>, which means that <span class="math notranslate nohighlight">\((\lambda_2-\mu)\)</span> can be approximated by applying the Inverse Power Method to <span class="math notranslate nohighlight">\((A-\mu I)\)</span>.</p>
<p>We demonstrate the computation of the middle eigenvalue of the matrix from the previous example.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{equation}
A = \left[ \begin{array}{rrrr} 9 &amp; -1 &amp; -3 \\ 0 &amp; 6 &amp; 0 \\ -6 &amp; 3 &amp; 6 \end{array}\right]
\end{equation}
\end{split}\]</div>
<p>By using the Inverse Power Method we determined that the smallest eigenvalue of <span class="math notranslate nohighlight">\(A\)</span> is 3.  Applying the Power Method directly will show that the largest eigenvalue of <span class="math notranslate nohighlight">\(A\)</span> is 12.  Since the third eigenvalue must lie somewhere in between these extremes, we choose <span class="math notranslate nohighlight">\(\mu\)</span> to be exactly in the middle at <span class="math notranslate nohighlight">\(7.5\)</span>.  Note that once we have a good approximation to the eigenvector with <span class="math notranslate nohighlight">\(X^{(m)}\)</span>, we can approximate the eigenvalue of <span class="math notranslate nohighlight">\(A\)</span> with <span class="math notranslate nohighlight">\(||AX^{(m)}||\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">],[</span><span class="mi">1</span><span class="p">],[</span><span class="mi">0</span><span class="p">]])</span>

<span class="n">m</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">tolerance</span> <span class="o">=</span> <span class="mf">0.0001</span>
<span class="n">MAX_ITERATIONS</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">difference</span> <span class="o">=</span> <span class="n">X</span>

<span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">9</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">3</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">0</span><span class="p">],[</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">6</span><span class="p">]])</span>
<span class="n">I</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="n">mu</span> <span class="o">=</span> <span class="mf">7.5</span>
<span class="n">Shifted_A</span> <span class="o">=</span> <span class="n">A</span><span class="o">-</span><span class="n">mu</span><span class="o">*</span><span class="n">I</span>
<span class="n">LU_factorization</span> <span class="o">=</span> <span class="n">sla</span><span class="o">.</span><span class="n">lu_factor</span><span class="p">(</span><span class="n">Shifted_A</span><span class="p">)</span>

<span class="k">while</span> <span class="p">(</span><span class="n">m</span> <span class="o">&lt;</span> <span class="n">MAX_ITERATIONS</span> <span class="ow">and</span> <span class="n">lag</span><span class="o">.</span><span class="n">Magnitude</span><span class="p">(</span><span class="n">difference</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">tolerance</span><span class="p">):</span>
    <span class="n">X_previous</span> <span class="o">=</span> <span class="n">X</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">sla</span><span class="o">.</span><span class="n">lu_solve</span><span class="p">(</span><span class="n">LU_factorization</span><span class="p">,</span><span class="n">X</span><span class="p">)</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">/</span><span class="n">lag</span><span class="o">.</span><span class="n">Magnitude</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

    <span class="c1">## Compute difference in stopping condition</span>
    <span class="n">difference</span> <span class="o">=</span> <span class="n">X</span> <span class="o">-</span> <span class="n">X_previous</span>
    
    <span class="n">m</span> <span class="o">=</span> <span class="n">m</span> <span class="o">+</span> <span class="mi">1</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Eigenvector is approximately:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Eigenvalue is A is approximately:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">lag</span><span class="o">.</span><span class="n">Magnitude</span><span class="p">(</span><span class="n">A</span><span class="nd">@X</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Eigenvector is approximately:
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[0.44232587]
 [0.88465174]
 [0.14744196]] 

Eigenvalue is A is approximately:
6.0
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="exercises">
<h2>Exercises<a class="headerlink" href="#exercises" title="Permalink to this headline">¶</a></h2>
<p>Let <span class="math notranslate nohighlight">\(A\)</span> be the matrix from the Inverse Power Method example.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{equation}
A = \left[ \begin{array}{rrr} 9 &amp; -1 &amp; -3 \\ 0 &amp; 6 &amp; 0 \\ -6 &amp; 3 &amp; 6 \end{array}\right]
\end{equation}
\end{split}\]</div>
<ul class="simple">
<li><p>Use the Power Method to approximate the largest eigenvalue <span class="math notranslate nohighlight">\(\lambda_1\)</span>.  Verify that the exact value of <span class="math notranslate nohighlight">\(\lambda_1\)</span> is 12.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">9</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">3</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">0</span><span class="p">],[</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">6</span><span class="p">]])</span>

<span class="c1">## Code solution here.</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Apply the Inverse Power Method with a shift of <span class="math notranslate nohighlight">\(\mu = 10\)</span>.  Explain why the results differ from those in the example.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">## Code solution here.</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Apply the Inverse Power Method with a shift 0f <span class="math notranslate nohighlight">\(\mu = 7.5\)</span> and the initial vector given below.  Explain why the sequence of vectors approach the eigenvector corresponding to <span class="math notranslate nohighlight">\(\lambda_1\)</span></p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{equation}
X^{(0)} = \left[ \begin{array}{r} 1 \\ 0  \\ 0 \end{array}\right]
\end{equation}
\end{split}\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">## Code solution here.</span>
</pre></div>
</div>
</div>
</div>
<p>Let <span class="math notranslate nohighlight">\(B\)</span> be the following matrix.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{equation}
B = \left[ \begin{array}{rrrr} -2 &amp; -18 &amp; 6 \\ -11 &amp; 3 &amp; 11 \\ -27 &amp; 15 &amp; 31 \end{array}\right]
\end{equation}
\end{split}\]</div>
<ul class="simple">
<li><p>Apply the Power Method and Inverse Power Method with shifts to approximate all eigenvalues of the matrix <span class="math notranslate nohighlight">\(B\)</span>. (<em>Note that one of the eigenvalues of this matrix is negative.</em>)</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">## Code solution here.</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Check your results using the <span class="math notranslate nohighlight">\(\texttt{eig}\)</span> function in SciPy.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">## Code solution here.</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Burden, Richard L. et al. <em>Numerical Analysis</em>. 10th ed., Cengage Learning, 2014</p></li>
<li><p>Golub, Gene H. and Charles F. Van Loan. <em>Matrix Computations</em>., The Johns Hopkins University Press, 1989</p></li>
</ul>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "bvanderlei/jupyter-guide-to-linear-algebra",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="Diagonalization.html" title="previous page">Diagonalization</a>
    <a class='right-next' id="next-link" href="Applications_EV.html" title="next page">Applications of Eigenvalues and Eigenvectors</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Ben Vanderlei<br/>
        
          <div class="extra_footer">
            <a rel="license" href="http://creativecommons.org/licenses/by-nc/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc/4.0/88x31.png" /></a><br /><span xmlns:dct="http://purl.org/dc/terms/" property="dct:title">Jupyter Guide to Linear Algebra</span> is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nc/4.0/">Creative Commons Attribution-NonCommercial 4.0 International License</a>.
          </div>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="_static/js/index.d3f166471bb80abb5163.js"></script>


    
  </body>
</html>